{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Churn Modeling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import usual suspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RowNumber  CustomerId    Surname  CreditScore Geography  Gender  Age  \\\n",
       "0           1    15634602   Hargrave          619    France  Female   42   \n",
       "1           2    15647311       Hill          608     Spain  Female   41   \n",
       "2           3    15619304       Onio          502    France  Female   42   \n",
       "3           4    15701354       Boni          699    France  Female   39   \n",
       "4           5    15737888   Mitchell          850     Spain  Female   43   \n",
       "5           6    15574012        Chu          645     Spain    Male   44   \n",
       "6           7    15592531   Bartlett          822    France    Male   50   \n",
       "7           8    15656148     Obinna          376   Germany  Female   29   \n",
       "8           9    15792365         He          501    France    Male   44   \n",
       "9          10    15592389         H?          684    France    Male   27   \n",
       "10         11    15767821     Bearce          528    France    Male   31   \n",
       "11         12    15737173    Andrews          497     Spain    Male   24   \n",
       "12         13    15632264        Kay          476    France  Female   34   \n",
       "13         14    15691483       Chin          549    France  Female   25   \n",
       "14         15    15600882      Scott          635     Spain  Female   35   \n",
       "15         16    15643966    Goforth          616   Germany    Male   45   \n",
       "16         17    15737452      Romeo          653   Germany    Male   58   \n",
       "17         18    15788218  Henderson          549     Spain  Female   24   \n",
       "18         19    15661507    Muldrow          587     Spain    Male   45   \n",
       "19         20    15568982        Hao          726    France  Female   24   \n",
       "\n",
       "    Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0        2       0.00              1          1               1   \n",
       "1        1   83807.86              1          0               1   \n",
       "2        8  159660.80              3          1               0   \n",
       "3        1       0.00              2          0               0   \n",
       "4        2  125510.82              1          1               1   \n",
       "5        8  113755.78              2          1               0   \n",
       "6        7       0.00              2          1               1   \n",
       "7        4  115046.74              4          1               0   \n",
       "8        4  142051.07              2          0               1   \n",
       "9        2  134603.88              1          1               1   \n",
       "10       6  102016.72              2          0               0   \n",
       "11       3       0.00              2          1               0   \n",
       "12      10       0.00              2          1               0   \n",
       "13       5       0.00              2          0               0   \n",
       "14       7       0.00              2          1               1   \n",
       "15       3  143129.41              2          0               1   \n",
       "16       1  132602.88              1          1               0   \n",
       "17       9       0.00              2          1               1   \n",
       "18       6       0.00              1          0               0   \n",
       "19       6       0.00              2          1               1   \n",
       "\n",
       "    EstimatedSalary  Exited  \n",
       "0         101348.88       1  \n",
       "1         112542.58       0  \n",
       "2         113931.57       1  \n",
       "3          93826.63       0  \n",
       "4          79084.10       0  \n",
       "5         149756.71       1  \n",
       "6          10062.80       0  \n",
       "7         119346.88       1  \n",
       "8          74940.50       0  \n",
       "9          71725.73       0  \n",
       "10         80181.12       0  \n",
       "11         76390.01       0  \n",
       "12         26260.98       0  \n",
       "13        190857.79       0  \n",
       "14         65951.65       0  \n",
       "15         64327.26       0  \n",
       "16          5097.67       1  \n",
       "17         14406.41       0  \n",
       "18        158684.81       0  \n",
       "19         54724.03       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>9981</td>\n",
       "      <td>15719276</td>\n",
       "      <td>T'ao</td>\n",
       "      <td>741</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>74371.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99595.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>9982</td>\n",
       "      <td>15672754</td>\n",
       "      <td>Burbidge</td>\n",
       "      <td>498</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>152039.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53445.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>9983</td>\n",
       "      <td>15768163</td>\n",
       "      <td>Griffin</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>137145.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115146.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>9984</td>\n",
       "      <td>15656710</td>\n",
       "      <td>Cocci</td>\n",
       "      <td>613</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151325.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>9985</td>\n",
       "      <td>15696175</td>\n",
       "      <td>Echezonachukwu</td>\n",
       "      <td>602</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>90602.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51695.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>9987</td>\n",
       "      <td>15581736</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>673</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>183579.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34047.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>9988</td>\n",
       "      <td>15588839</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>606</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>180307.73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1914.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>9989</td>\n",
       "      <td>15589329</td>\n",
       "      <td>Pirozzi</td>\n",
       "      <td>775</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49337.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9990</td>\n",
       "      <td>15605622</td>\n",
       "      <td>McMillan</td>\n",
       "      <td>841</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179436.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>9991</td>\n",
       "      <td>15798964</td>\n",
       "      <td>Nkemakonam</td>\n",
       "      <td>714</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>35016.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53667.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>9992</td>\n",
       "      <td>15769959</td>\n",
       "      <td>Ajuluchukwu</td>\n",
       "      <td>597</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>88381.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69384.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>9993</td>\n",
       "      <td>15657105</td>\n",
       "      <td>Chukwualuka</td>\n",
       "      <td>726</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195192.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>9994</td>\n",
       "      <td>15569266</td>\n",
       "      <td>Rahman</td>\n",
       "      <td>644</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>155060.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29179.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9995</td>\n",
       "      <td>15719294</td>\n",
       "      <td>Wood</td>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId         Surname  CreditScore Geography  Gender  \\\n",
       "9980       9981    15719276            T'ao          741     Spain    Male   \n",
       "9981       9982    15672754        Burbidge          498   Germany    Male   \n",
       "9982       9983    15768163         Griffin          655   Germany  Female   \n",
       "9983       9984    15656710           Cocci          613    France    Male   \n",
       "9984       9985    15696175  Echezonachukwu          602   Germany    Male   \n",
       "9985       9986    15586914          Nepean          659    France    Male   \n",
       "9986       9987    15581736        Bartlett          673   Germany    Male   \n",
       "9987       9988    15588839         Mancini          606     Spain    Male   \n",
       "9988       9989    15589329         Pirozzi          775    France    Male   \n",
       "9989       9990    15605622        McMillan          841     Spain    Male   \n",
       "9990       9991    15798964      Nkemakonam          714   Germany    Male   \n",
       "9991       9992    15769959     Ajuluchukwu          597    France  Female   \n",
       "9992       9993    15657105     Chukwualuka          726     Spain    Male   \n",
       "9993       9994    15569266          Rahman          644    France    Male   \n",
       "9994       9995    15719294            Wood          800    France  Female   \n",
       "9995       9996    15606229        Obijiaku          771    France    Male   \n",
       "9996       9997    15569892       Johnstone          516    France    Male   \n",
       "9997       9998    15584532             Liu          709    France  Female   \n",
       "9998       9999    15682355       Sabbatini          772   Germany    Male   \n",
       "9999      10000    15628319          Walker          792    France  Female   \n",
       "\n",
       "      Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "9980   35       6   74371.49              1          0               0   \n",
       "9981   42       3  152039.70              1          1               1   \n",
       "9982   46       7  137145.12              1          1               0   \n",
       "9983   40       4       0.00              1          0               0   \n",
       "9984   35       7   90602.42              2          1               1   \n",
       "9985   36       6  123841.49              2          1               0   \n",
       "9986   47       1  183579.54              2          0               1   \n",
       "9987   30       8  180307.73              2          1               1   \n",
       "9988   30       4       0.00              2          1               0   \n",
       "9989   28       4       0.00              2          1               1   \n",
       "9990   33       3   35016.60              1          1               0   \n",
       "9991   53       4   88381.21              1          1               0   \n",
       "9992   36       2       0.00              1          1               0   \n",
       "9993   28       7  155060.41              1          1               0   \n",
       "9994   29       2       0.00              2          0               0   \n",
       "9995   39       5       0.00              2          1               0   \n",
       "9996   35      10   57369.61              1          1               1   \n",
       "9997   36       7       0.00              1          0               1   \n",
       "9998   42       3   75075.31              2          1               0   \n",
       "9999   28       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "9980         99595.67       0  \n",
       "9981         53445.17       1  \n",
       "9982        115146.40       1  \n",
       "9983        151325.24       0  \n",
       "9984         51695.41       0  \n",
       "9985         96833.00       0  \n",
       "9986         34047.54       0  \n",
       "9987          1914.41       0  \n",
       "9988         49337.84       0  \n",
       "9989        179436.60       0  \n",
       "9990         53667.08       0  \n",
       "9991         69384.71       1  \n",
       "9992        195192.40       0  \n",
       "9993         29179.52       0  \n",
       "9994        167773.55       0  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating X by defining all indepdendent variables from dataset and using it values as numpy array\n",
    "X = dataset.iloc[:,3:13].values\n",
    "\n",
    "# Creating y by defining dependent variable from dataset and using it values as numpy array\n",
    "y = dataset.iloc[:,13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' 42 2 0.0 1 1 1 101348.88]\n",
      " [608 'Spain' 'Female' 41 1 83807.86 1 0 1 112542.58]\n",
      " [502 'France' 'Female' 42 8 159660.8 3 1 0 113931.57]\n",
      " [699 'France' 'Female' 39 1 0.0 2 0 0 93826.63]\n",
      " [850 'Spain' 'Female' 43 2 125510.82 1 1 1 79084.1]\n",
      " [645 'Spain' 'Male' 44 8 113755.78 2 1 0 149756.71]\n",
      " [822 'France' 'Male' 50 7 0.0 2 1 1 10062.8]\n",
      " [376 'Germany' 'Female' 29 4 115046.74 4 1 0 119346.88]\n",
      " [501 'France' 'Male' 44 4 142051.07 2 0 1 74940.5]]\n"
     ]
    }
   ],
   "source": [
    "# Let's see first 5 rows\n",
    "print (X[0:9,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Binary variable showing 1 == yes, customer exited the bank and 0 == no, custome didn't exited bank\n",
    "print (y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, since our dataset contains few categorical variables with string/character dtypes, we need to do encoding to convert them into numerical dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's use sklearn preprocessing based LabelEncode and OneHotEncoder method to do this\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# get copy of X to do encoding\n",
    "# get copy of y to do encoding\n",
    "X_encoded = X\n",
    "y_encoded = y\n",
    "\n",
    "# now, let's do encoding for country categorical variable at 1 position\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X_encoded[:,1] = labelencoder_X_1.fit_transform(X_encoded[:,1])\n",
    "\n",
    "# encoding for Sex categorical variable at 2 position\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X_encoded[:,2] = labelencoder_X_2.fit_transform(X_encoded[:,2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, IMPORTANT, we know our categorical variables are not ordinal type mean they don't have any order so country france = 0, spain = 2, germany = 1 has no order\n",
    "\n",
    "### To acheive above, we need to use one hot encoding method from sklearn preprocessing, which creates dummy variables for each category type of data so, 3 dummy columns will be created aka dummy_france_0, dummy_spain_2 and dummy_germany_1 \n",
    "\n",
    "### and wherever their is true value in those dummy columns, 1 will be putted as one hot encoding ideally same for sex and 0 = female and 1 = male has no order but since it's only 0 and 1, sex variable can be treated as binary variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10000, 12)\n",
      "[[1.00 0.00 0.00 619.00 0.00 42.00 2.00 0.00 1.00 1.00 1.00 101348.88]\n",
      " [0.00 0.00 1.00 608.00 0.00 41.00 1.00 83807.86 1.00 0.00 1.00 112542.58]\n",
      " [1.00 0.00 0.00 502.00 0.00 42.00 8.00 159660.80 3.00 1.00 0.00 113931.57]\n",
      " [1.00 0.00 0.00 699.00 0.00 39.00 1.00 0.00 2.00 0.00 0.00 93826.63]\n",
      " [0.00 0.00 1.00 850.00 0.00 43.00 2.00 125510.82 1.00 1.00 1.00 79084.10]\n",
      " [0.00 0.00 1.00 645.00 1.00 44.00 8.00 113755.78 2.00 1.00 0.00 149756.71]\n",
      " [1.00 0.00 0.00 822.00 1.00 50.00 7.00 0.00 2.00 1.00 1.00 10062.80]\n",
      " [0.00 1.00 0.00 376.00 0.00 29.00 4.00 115046.74 4.00 1.00 0.00 119346.88]\n",
      " [1.00 0.00 0.00 501.00 1.00 44.00 4.00 142051.07 2.00 0.00 1.00 74940.50]]\n"
     ]
    }
   ],
   "source": [
    "# so let's use OneHotEncoder method and give it to only country variable at index position 1\n",
    "onehotencoder = OneHotEncoder(categorical_features=[1])\n",
    "# now, use fit transform to apply one hot encoding and then cast it into numpy array\n",
    "X_encoded = onehotencoder.fit_transform(X_encoded).toarray()\n",
    "\n",
    "print (type(X_encoded))\n",
    "print (X_encoded.shape)\n",
    "print (X_encoded[0:9,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can u see first 3 columns as dummy columns for dummy_france_0, dummy_germany_1 and dummy_spain_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, IMPORTANT: to handle dummy variable trap, we need to drop one of 3 newly created dummy column\n",
    "### Why because, we only need 2 dummy varibles to represent 3 countries true and false values\n",
    "### meaning if values in dummy_germany_1 is 0.00 and dummy_spain_2 is 0.00 it mean that that row obviously belong to france, so we don't need dummy_france_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_encoded <class 'numpy.ndarray'>\n",
      "(10000, 11)\n",
      "[[0.00 0.00 619.00 0.00 42.00 2.00 0.00 1.00 1.00 1.00 101348.88]\n",
      " [0.00 1.00 608.00 0.00 41.00 1.00 83807.86 1.00 0.00 1.00 112542.58]\n",
      " [0.00 0.00 502.00 0.00 42.00 8.00 159660.80 3.00 1.00 0.00 113931.57]\n",
      " [0.00 0.00 699.00 0.00 39.00 1.00 0.00 2.00 0.00 0.00 93826.63]\n",
      " [0.00 1.00 850.00 0.00 43.00 2.00 125510.82 1.00 1.00 1.00 79084.10]\n",
      " [0.00 1.00 645.00 1.00 44.00 8.00 113755.78 2.00 1.00 0.00 149756.71]\n",
      " [0.00 0.00 822.00 1.00 50.00 7.00 0.00 2.00 1.00 1.00 10062.80]\n",
      " [1.00 0.00 376.00 0.00 29.00 4.00 115046.74 4.00 1.00 0.00 119346.88]\n",
      " [0.00 0.00 501.00 1.00 44.00 4.00 142051.07 2.00 0.00 1.00 74940.50]]\n"
     ]
    }
   ],
   "source": [
    "# so let's drop first column dummy_france_0 as 2 dummy columns are sufficient to reprsent 3 countries presense in dataset\n",
    "X_encoded = X_encoded[:,1:]\n",
    "\n",
    "print (\"X_encoded\", type(X_encoded))\n",
    "print (X_encoded.shape)\n",
    "print (X_encoded[0:9,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, let's Split encoded X and y into Training set and Test set via sklearn model selection train_test_split method\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see shape and sample from X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train <class 'numpy.ndarray'>\n",
      "(8000, 11)\n",
      "[[0.00 1.00 667.00 0.00 34.00 5.00 0.00 2.00 1.00 0.00 163830.64]\n",
      " [1.00 0.00 427.00 1.00 42.00 1.00 75681.52 1.00 1.00 1.00 57098.00]\n",
      " [0.00 0.00 535.00 0.00 29.00 2.00 112367.34 1.00 1.00 0.00 185630.76]\n",
      " [0.00 1.00 654.00 1.00 40.00 5.00 105683.63 1.00 1.00 0.00 173617.09]\n",
      " [0.00 1.00 850.00 0.00 57.00 8.00 126776.30 2.00 1.00 1.00 132298.49]\n",
      " [1.00 0.00 776.00 0.00 37.00 2.00 103769.22 2.00 1.00 0.00 194099.12]\n",
      " [0.00 0.00 807.00 1.00 47.00 1.00 95120.59 1.00 0.00 0.00 127875.10]\n",
      " [0.00 1.00 598.00 1.00 41.00 8.00 0.00 2.00 1.00 1.00 161954.43]\n",
      " [0.00 1.00 636.00 1.00 76.00 9.00 126534.60 1.00 1.00 1.00 39789.62]]\n",
      "\n",
      " X_test <class 'numpy.ndarray'>\n",
      "(2000, 11)\n",
      "[[1.00 0.00 597.00 0.00 35.00 8.00 131101.04 1.00 1.00 1.00 192852.67]\n",
      " [0.00 0.00 523.00 0.00 40.00 2.00 102967.41 1.00 1.00 0.00 128702.10]\n",
      " [0.00 1.00 706.00 0.00 42.00 8.00 95386.82 1.00 1.00 1.00 75732.25]\n",
      " [0.00 0.00 788.00 1.00 32.00 4.00 112079.58 1.00 0.00 0.00 89368.59]\n",
      " [1.00 0.00 706.00 1.00 38.00 5.00 163034.82 2.00 1.00 1.00 135662.17]\n",
      " [0.00 1.00 670.00 0.00 57.00 3.00 175575.95 2.00 1.00 0.00 99061.75]\n",
      " [0.00 1.00 590.00 1.00 34.00 0.00 65812.35 2.00 0.00 1.00 160346.30]\n",
      " [0.00 1.00 636.00 0.00 29.00 6.00 157576.47 2.00 1.00 1.00 101102.39]\n",
      " [0.00 0.00 598.00 0.00 64.00 9.00 0.00 1.00 0.00 1.00 13181.37]]\n"
     ]
    }
   ],
   "source": [
    "print (\"X_train\", type(X_train))\n",
    "print (X_train.shape)\n",
    "print (X_train[0:9,:])\n",
    "\n",
    "print (\"\\n X_test\", type(X_test))\n",
    "print (X_test.shape)\n",
    "print (X_test[0:9,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, IMPORTANT: let's do Feature Scaling for our future ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(8000, 11)\n",
      "[[-0.57 1.74 0.17 -1.09 -0.46 0.01 -1.22 0.81 0.64 -1.03 1.11]\n",
      " [1.75 -0.57 -2.30 0.92 0.30 -1.38 -0.01 -0.92 0.64 0.97 -0.75]\n",
      " [-0.57 -0.57 -1.19 -1.09 -0.94 -1.03 0.58 -0.92 0.64 -1.03 1.49]\n",
      " [-0.57 1.74 0.04 0.92 0.11 0.01 0.47 -0.92 0.64 -1.03 1.28]\n",
      " [-0.57 1.74 2.06 -1.09 1.74 1.04 0.81 0.81 0.64 0.97 0.56]\n",
      " [1.75 -0.57 1.29 -1.09 -0.18 -1.03 0.44 0.81 0.64 -1.03 1.63]\n",
      " [-0.57 -0.57 1.61 0.92 0.78 -1.38 0.30 -0.92 -1.56 -1.03 0.48]\n",
      " [-0.57 1.74 -0.54 0.92 0.21 1.04 -1.22 0.81 0.64 0.97 1.07]\n",
      " [-0.57 1.74 -0.15 0.92 3.55 1.39 0.81 -0.92 0.64 0.97 -1.05]]\n",
      "<class 'numpy.ndarray'>\n",
      "(2000, 11)\n",
      "[[1.63 -0.57 -0.56 -1.11 -0.39 0.99 0.86 -0.87 0.66 0.98 1.62]\n",
      " [-0.61 -0.57 -1.34 -1.11 0.08 -1.08 0.40 -0.87 0.66 -1.02 0.50]\n",
      " [-0.61 1.74 0.58 -1.11 0.26 0.99 0.28 -0.87 0.66 0.98 -0.42]\n",
      " [-0.61 -0.57 1.44 0.90 -0.68 -0.39 0.55 -0.87 -1.51 -1.02 -0.18]\n",
      " [1.63 -0.57 0.58 0.90 -0.11 -0.05 1.38 0.80 0.66 0.98 0.63]\n",
      " [-0.61 1.74 0.21 -1.11 1.67 -0.74 1.58 0.80 0.66 -1.02 -0.01]\n",
      " [-0.61 1.74 -0.63 0.90 -0.49 -1.77 -0.20 0.80 -1.51 0.98 1.06]\n",
      " [-0.61 1.74 -0.15 -1.11 -0.96 0.30 1.29 0.80 0.66 0.98 0.02]\n",
      " [-0.61 -0.57 -0.55 -1.11 2.33 1.33 -1.27 -0.87 -1.51 0.98 -1.51]]\n"
     ]
    }
   ],
   "source": [
    "# we will use here sklearn preprocessing StandardScaler method to do feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "print (type(X_train))\n",
    "print (X_train.shape)\n",
    "print (X_train[0:9,:])\n",
    "print (type(X_test))\n",
    "print (X_test.shape)\n",
    "print (X_test[0:9,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you see how all long range integer features values like credit score at index 2 and Age at index 4 and Tenure at index 5 and Balance at index 6 and Estimated Salary at index 10 are all now in lower range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA IS PREPROCESSED NOW AND READY FOR ANN MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the keras libaray and make sure to use Tensorflow background\n",
    "\n",
    "### to change keras background make sure to edit\n",
    "\n",
    "```\n",
    "nano ~/.keras/keras.json\n",
    "```\n",
    "\n",
    "### and make sure it look like below\n",
    "\n",
    "```\n",
    "{\n",
    "    \"image_dim_ordering\": \"tf\", \n",
    "    \"epsilon\": 1e-07, \n",
    "    \"floatx\": \"float32\", \n",
    "    \"backend\": \"tensorflow\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's use keras sequential module to initialize sequential (not a graph) ANN\n",
    "from keras.models import Sequential\n",
    "\n",
    "# and dense layer to build layers\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, To build and training a Ann with Stochastic Gradient Descent, we will follow step by step\n",
    "\n",
    "# Step 1\n",
    "# Weight Initialization:\n",
    "# Randomly Initialize the weights to small numbers close to 0 (but not 0)\n",
    "\n",
    "# Step 2\n",
    "# Input Assigenment:\n",
    "# Input the first observation of your dataset in the input layer, each feature is one input node\n",
    "\n",
    "# Step 3\n",
    "# Forward-Propogation:\n",
    "# from left to right, the neurons are activated in a way that the impact of each neurons'\n",
    "# activation is limited by the weights. \n",
    "# Propogate the activation until getting the predicted results y\n",
    "\n",
    "# Step 4\n",
    "# Cost Calculation:\n",
    "# Compare the predicted result to actual result. Measure the generated error.\n",
    "\n",
    "# Step 5\n",
    "# Back Propogation:\n",
    "# From right to left, the error is backpropogated. Updates the weight according to how much proportionally\n",
    "# they are responsible for the error. \n",
    "\n",
    "# Step 6\n",
    "# Batch Processing:\n",
    "# Repeat Step 1 to 5 and update the weights after a batch of observations\n",
    "# Use learning rate to decide how much gradually in each batch a weight should update\n",
    "\n",
    "# Step 7\n",
    "# Epoch Training:\n",
    "# When the whole training set passed through the ANN, that makes an epoch. \n",
    "# Redo the whole training again and again on many epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Now, let's initialize sequential based model (or graph based model)\n",
    "#  The Sequential model is a linear stack of layers.\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can also simply add layers via the .add() method\n",
    "\n",
    "# Adding the Input layer and the first hidden layer in Dense method\n",
    "# choosing unit =6 is an art, basically u average total dimensions of output and input layer\n",
    "# so, total dim in input layer is 11 and total dim in output layer is 1, so (11 + 1) / number of layers = 12/2 = 6\n",
    "classifier.add(Dense(units=6, kernel_initializer= 'glorot_uniform' , activation='relu', input_shape=(11,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units=6, kernel_initializer= 'glorot_uniform' , activation='relu', input_shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "classifier.add(Dense(units=1, kernel_initializer= 'glorot_uniform' , activation='sigmoid', input_shape=(6,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "# Before training a model, you need to configure the learning process, which is done via the compile method.\n",
    "# It receives three arguments:\n",
    "# An optimizer. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class. See: optimizers.\n",
    "# A loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function. See: losses.\n",
    "# A list of metrics. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function.\n",
    "\n",
    "#  Compling the ANN\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.6820 - acc: 0.6015     \n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.5722 - acc: 0.7975     \n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.5166 - acc: 0.7975     \n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4822 - acc: 0.7990     \n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4548 - acc: 0.8020     \n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4360 - acc: 0.8085     \n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4242 - acc: 0.8160     \n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4150 - acc: 0.8215     \n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4076 - acc: 0.8240     \n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.4010 - acc: 0.8255     \n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3950 - acc: 0.8310     \n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3895 - acc: 0.8305     \n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3839 - acc: 0.8320     \n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3783 - acc: 0.8425     \n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3733 - acc: 0.8470     \n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3681 - acc: 0.8505     \n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3631 - acc: 0.8525     \n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3597 - acc: 0.8550     \n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3559 - acc: 0.8575     \n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3525 - acc: 0.8565     \n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3490 - acc: 0.8585     \n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3464 - acc: 0.8595     \n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3441 - acc: 0.8625     \n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3422 - acc: 0.8625     \n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3405 - acc: 0.8605     \n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3383 - acc: 0.8645     \n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3371 - acc: 0.8620     \n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3359 - acc: 0.8645     \n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3355 - acc: 0.8645     - ETA: 0s - loss: 0.3408 - acc:\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3342 - acc: 0.8635     \n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3325 - acc: 0.8640     \n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3324 - acc: 0.8660     \n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3312 - acc: 0.8655     \n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3302 - acc: 0.8660     \n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3295 - acc: 0.8670     \n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3293 - acc: 0.8670     \n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3284 - acc: 0.8695     \n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3276 - acc: 0.8655     \n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3270 - acc: 0.8685     \n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3270 - acc: 0.8700     \n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3266 - acc: 0.8670     \n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3257 - acc: 0.8685     \n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3256 - acc: 0.8685     \n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3252 - acc: 0.8700     \n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3248 - acc: 0.8690     \n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3248 - acc: 0.8690     \n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3252 - acc: 0.8710     \n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3242 - acc: 0.8710     \n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3241 - acc: 0.8705     \n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3236 - acc: 0.8675     \n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3229 - acc: 0.8685     \n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3235 - acc: 0.8685     \n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3226 - acc: 0.8700     \n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3225 - acc: 0.8685     \n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3225 - acc: 0.8685     \n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3228 - acc: 0.8690     \n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3226 - acc: 0.8680     \n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3215 - acc: 0.8710     \n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3216 - acc: 0.8705     \n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3214 - acc: 0.8680     \n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3213 - acc: 0.8685     \n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3218 - acc: 0.8715     \n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3210 - acc: 0.8710     \n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3209 - acc: 0.8715     \n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3210 - acc: 0.8685     \n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3209 - acc: 0.8690     \n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3204 - acc: 0.8710     \n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3205 - acc: 0.8695     \n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3210 - acc: 0.8660     \n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3198 - acc: 0.8670     \n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3198 - acc: 0.8680     \n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3195 - acc: 0.8715     \n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3194 - acc: 0.8715     \n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3191 - acc: 0.8700     \n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3185 - acc: 0.8685     \n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3186 - acc: 0.8700     \n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3186 - acc: 0.8690     \n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3180 - acc: 0.8685     \n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3187 - acc: 0.8700     \n",
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3181 - acc: 0.8710     \n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3178 - acc: 0.8695     \n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3179 - acc: 0.8675     \n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3180 - acc: 0.8680     \n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3177 - acc: 0.8690     \n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3176 - acc: 0.8685     \n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3171 - acc: 0.8675     \n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s - loss: 0.3165 - acc: 0.8700     \n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3163 - acc: 0.8685     \n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3160 - acc: 0.8705     \n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3163 - acc: 0.8695     \n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3160 - acc: 0.8680     \n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3162 - acc: 0.8690     \n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3158 - acc: 0.8700     \n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3157 - acc: 0.8705     \n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3149 - acc: 0.8725     \n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3155 - acc: 0.8715     \n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3151 - acc: 0.8725     \n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3151 - acc: 0.8720     \n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3147 - acc: 0.8705     \n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s - loss: 0.3139 - acc: 0.8710     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11df1a8d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "# Keras models are trained on Numpy arrays of input data and labels.\n",
    "# For training a model, you will typically use the  fit function.\n",
    "\n",
    "classifier.fit(X_test, y_test, batch_size= 10 ,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  32/2000 [..............................] - ETA: 1s"
     ]
    }
   ],
   "source": [
    "score = classifier.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31067434525489807, 0.87150000000000005]\n"
     ]
    }
   ],
   "source": [
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see confusion metrix\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20]\n",
      " [0.41]\n",
      " [0.14]\n",
      " ..., \n",
      " [0.10]\n",
      " [0.16]\n",
      " [0.18]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "print (y_pred)\n",
    "print (type(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ..., \n",
      " [False]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_pred > 0.5)\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1539   56]\n",
      " [ 201  204]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1539   56]\n",
      "[[201 204]]\n"
     ]
    }
   ],
   "source": [
    "#  Calculate Accuracy\n",
    "print (cm[[0][0]])\n",
    "print (cm[[1]])\n",
    "# print ((cm[[0,0]]+cm[[1,1]])/(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, Predict a single customer with given data, whether he is goint to leave or stay in bank\n",
    "\n",
    "### Here is data\n",
    "\n",
    "Geography: France\n",
    "\n",
    "Credit Score: 600\n",
    "\n",
    "Gender: Male\n",
    "\n",
    "Age: 40 years old\n",
    "\n",
    "Tenure: 3 years\n",
    "\n",
    "Balance: $60000\n",
    "\n",
    "Number of Products: 2\n",
    "\n",
    "Does this customer have a credit card ? Yes\n",
    "\n",
    "Is this customer an Active Member: Yes\n",
    "\n",
    "Estimated Salary: $50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print (type(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.00 0.00 619.00 0.00 42.00 2.00 0.00 1.00 1.00 1.00 101348.88]\n",
    "\n",
    " [0.00 1.00 608.00 0.00 41.00 1.00 83807.86 1.00 0.00 1.00 112542.58]\n",
    " \n",
    " [0.00 0.00 502.00 0.00 42.00 8.00 159660.80 3.00 1.00 0.00 113931.57]\n",
    " \n",
    " [0.00 0.00 699.00 0.00 39.00 1.00 0.00 2.00 0.00 0.00 93826.63]\n",
    " \n",
    " [0.00 1.00 850.00 0.00 43.00 2.00 125510.82 1.00 1.00 1.00 79084.10]\n",
    " \n",
    " [0.00 1.00 645.00 1.00 44.00 8.00 113755.78 2.00 1.00 0.00 149756.71]\n",
    " \n",
    " [0.00 0.00 822.00 1.00 50.00 7.00 0.00 2.00 1.00 1.00 10062.80]\n",
    " \n",
    " [1.00 0.00 376.00 0.00 29.00 4.00 115046.74 4.00 1.00 0.00 119346.88]\n",
    " \n",
    " [0.00 0.00 501.00 1.00 44.00 4.00 142051.07 2.00 0.00 1.00 74940.50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00 0.00 600.00 1.00 40.00 3.00 60000.00 2.00 1.00 1.00 50000.00]]\n"
     ]
    }
   ],
   "source": [
    "# let's create 2 dimension numpy array with one row representing this customer\n",
    "X_sample = np.array([[0.0,0.0,600,1,40,3,60000,2,1,1,50000]])\n",
    "print (X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.61 -0.57 -0.53 0.90 0.08 -0.74 -0.29 0.80 0.66 0.98 -0.87]]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT since our model is trained on scaled values of X\n",
    "# let's scale X_sample numpy array before we use this for predection with model,\n",
    "X_sample = sc.transform(X_sample)\n",
    "print (X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02]]\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "# Now, let's predict, whether this customer will leave or remain on bank\n",
    "y_sample = classifier.predict(X_sample)\n",
    "print (y_sample)\n",
    "y_sample = (y_sample > 0.5)\n",
    "print (y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# so, False, customer will not leave the bank"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
